{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFRkJ+GfnPuTchsEMeDfjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mastercheifs/docs-l10n/blob/master/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5_%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8lUam9Xe_r0h"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "import tensorflow as tf\n",
        "\n",
        "class NN(tf.keras.Model):\n",
        "  def __init__(self, action_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.fc1 = Dense(24, activation='relu')\n",
        "    self.fc2 = Dense(24, activation='relu')\n",
        "    self.fc_out = Dense(action_size, kernel_initializer=RandomUniform(-1e-3, 1e-3))\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    q = self.fc_out(x)\n",
        "    return q\n",
        "\n",
        "class DQN:\n",
        "  def __init__(self, state_size, aciton_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = aciton_size\n",
        "\n",
        "    self.discount_factor = 0.99\n",
        "    self.learning_rate = 0.001\n",
        "    self.epsilon = 1.0\n",
        "    self.epsilon_decay = 0.999\n",
        "    self.epsilon_min = 0.001\n",
        "    self.batch_size = 64\n",
        "    self.train_start = 1000\n",
        "\n",
        "    self.memory = deque(maxlen=2000)\n",
        "\n",
        "    self.model = NN(self.action_size)\n",
        "    self.target_model = NN(self.action_size)\n",
        "    self.optimizer = Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "    self.update_target_model()\n",
        "\n",
        "  def update_target_model(self):\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "  def get_action(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return random.randrange(self.action_size)\n",
        "    else:\n",
        "      q = self.model(state) #리스트 형태로 반환됨\n",
        "      return np.argmax(q[0])\n",
        "\n",
        "  def append_sample(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def train_model(self):\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    mini_batch = random.sample(self.memory, self.batch_size)\n",
        "    states = np.array([sample[0][0] for sample in mini_batch])\n",
        "    actions = np.array([sample[1] for sample in mini_batch])\n",
        "    rewards = np.array([sample[2] for sample in mini_batch])\n",
        "    next_states = np.array([sample[3][0] for sample in mini_batch])\n",
        "    dones = np.array([sample[4] for sample in mini_batch])\n",
        "\n",
        "    model_params = self.model.trainable_variables\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicts = self.model(states)\n",
        "      one_hot_action = tf.one_hot(actions , self.action_size)\n",
        "      predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
        "\n",
        "      target_predicts = self.target_model(next_states)\n",
        "      target_predicts = tf.stop_gradient(target_predicts)\n",
        "\n",
        "      max_q = np.amax(target_predicts, axis=-1)\n",
        "      targets = rewards + (1-dones) * self.discount_factor * max_q\n",
        "      loss = tf.reduce_mean(tf.square(targets - predicts))\n",
        "\n",
        "    grads = tape.gradient(loss, model_params)\n",
        "    self.optimizer.apply_gradients(zip(grads, model_params))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym \n",
        "env = gym.make('Acrobot-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "agent = DQN(state_size, action_size)\n",
        "\n",
        "scores, episodes = [], []\n",
        "score_avg = 0\n",
        "\n",
        "EPISODES = 300\n",
        "\n",
        "for e in range(EPISODES):\n",
        "  done = False\n",
        "  score = 0\n",
        "\n",
        "  state = env.reset()\n",
        "  state = np.reshape(state, [1, state_size])\n",
        "\n",
        "  while not done:\n",
        "    action = agent.get_action(state)\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "    reward = -(next_state[0][0]) - (next_state[0][0]*next_state[0][1] - next_state[0][2]*next_state[0][3])\n",
        "    score += reward\n",
        "\n",
        "    agent.append_sample(state, action, reward, next_state, done)\n",
        "\n",
        "    if len(agent.memory) >= agent.train_start:\n",
        "      agent.train_model()\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    if done:\n",
        "      agent.update_target_model()\n",
        "\n",
        "      scores.append(score)\n",
        "      episodes.append(e)\n",
        "\n",
        "      if e % 10 == 0:  # 매 10 에피소드마다 평균 점수 계산\n",
        "        score_avg = np.mean(scores[-min(10, len(scores)):])\n",
        "        print(f\"에피소드 : {e} | 점수 : {score} | 평균 점수 : {score_avg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTkZlXBGBI6O",
        "outputId": "e45125d2-4f35-47e6-e120-ac3cc977427c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에피소드 : 0 | 점수 : -471.9214314520359 | 평균 점수 : -471.9214314520359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f93fa1acca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f93fa1acca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(episodes, scores)\n",
        "plt.title('Score over Episodes')\n",
        "plt.xlabel('literations')\n",
        "plt.ylabel('Average Return')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cmKAoj8HBI8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'Acrobot-v1'\n",
        "env = gym.make('Acrobot-v1')"
      ],
      "metadata": {
        "id": "g6fxAKupBI_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "metadata": {
        "id": "fWwH1IrM1AfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))\n"
      ],
      "metadata": {
        "id": "oDg_TC_9FErh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy(observation):\n",
        "    return agent.get_action(observation)\n",
        "\n",
        "create_policy_eval_video(policy, \"trained-agent\")\n"
      ],
      "metadata": {
        "id": "zbrX-6swfxCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "metadata": {
        "id": "Q84yfa3whGEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "metadata": {
        "id": "8RJFt91Snr_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "metadata": {
        "id": "IOAv9TWeW45i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ],
      "metadata": {
        "id": "rvN-QsyZW4pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(agent))\n"
      ],
      "metadata": {
        "id": "lc2Y2Oj2W4bx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}